{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/UNSW-COMP9418/Week10/blob/main/COMP9418_W10_Approximate_Inference_with_Sampling.ipynb)\n",
    "\n",
    "# Approximate Inference with Sampling\n",
    "\n",
    "**COMP9418 W10 Tutorial**\n",
    "\n",
    "- Instructor: Gustavo Batista\n",
    "- School of Computer Science and Engineering, UNSW Sydney\n",
    "- Notebook designed by Gustavo Batista\n",
    "- Last Update 6th September 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's tutorial, we will implement the four sampling algorithms discussed in the lecture. We will also investigate the performance of these algorithms in terms of running time and accuracy for answering marginal and conditional probabilistic queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "You will need certain packages installed to run this notebook.\n",
    "\n",
    "If you are using ``conda``'s default [full installation](https://conda.io/docs/install/full.html), these requirements should all be satisfied already.\n",
    "\n",
    "If you are using ``virtualenv`` or other native package management, you may need to run this command:\n",
    "\n",
    "```python\n",
    "pip3 install matplotlib\n",
    "```\n",
    "\n",
    "To render a visualization of some graphical models, you also need to install Graphviz [download page](http://www.graphviz.org/download). We have already used this library in Tutorial 1, thus, you should have it installed.\n",
    "\n",
    "Once we have done all that, we import some useful modules for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise plots\n",
    "import matplotlib.pyplot as plt\n",
    "# random number generator library\n",
    "import random\n",
    "# access to math.inf\n",
    "import math\n",
    "# ndarrays\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiscreteFactors import Factor\n",
    "from Graph import Graph\n",
    "from BayesNet import BayesNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample a Bayesian network variable\n",
    "\n",
    "First, we will develop a simple function to sample one variable $X$ according to $P(X|\\textbf{u})$, with $\\textbf{u}$ being a complete assignment for the variables $\\textbf{U}$ that are the parents of $X$.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's extend the `Factor` class by implementing the function `sample_variable` that samples a variable $X$ according to $P(X|\\textbf{u})$. The function will assume that the factor is appropriately normalized with $X$ as the child variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Factor(Factor):\n",
    "    def sample_variable(self, var, **evi):\n",
    "        '''\n",
    "        This function assumes that all domain variables are in `evi` except for `var`.\n",
    "        The factor must also be appropriately normalized.\n",
    "        `var`: variable to be sampled according to P(var|parents(var))\n",
    "        `evi`: dict of assignments of parent variables\n",
    "        return: value sampled according to P(var|parents(var))\n",
    "        '''\n",
    "        # Sample a value in the interval [0,1) using random()\n",
    "        r = random.random()\n",
    "        # s has the accumulative probabilities of P(var|parents(var))\n",
    "        s = 0\n",
    "        # select the appropriate row from the table\n",
    "        indicies = tuple(slice(None) if v==var else self.outcomeSpace[v].index(evi[v]) for v in self.domain)\n",
    "        # t will be the normalized vector of probs we are sampling from\n",
    "        t = self.table[indicies]\n",
    "        for i,outcome in enumerate(self.outcomeSpace[var]):\n",
    "            s += ... # TODO\n",
    "            # If we have achived a s value higher than s so v is the sampled value. \n",
    "            if r < s:\n",
    "                return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ICU network\n",
    "\n",
    "Once again, we will use the ICU-Alarm network as a benchmark. You should remember this Bayesian Network from Week 3 Tutorial. It is a subset of nine nodes from a [larger network](https://www.bnlearn.com/bnrepository/discrete-medium.html#alarm) of 37 nodes.\n",
    "\n",
    "The following image is a graphical representation of a subset of nine nodes we will use in this tutorial.\n",
    "\n",
    "![ICU Graph](img/ICU_graph.png \"Graph exercise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define Bayes Net Graph\n",
    "graph = Graph({\n",
    "    'L': ['S', 'V'],\n",
    "    'H': ['S', 'V'],\n",
    "    'S': ['O'],\n",
    "    'V': ['C', 'O'],\n",
    "    'O': ['B'],\n",
    "    'A': ['T'],\n",
    "    'T': ['B'],\n",
    "    'C': [],\n",
    "    'B': [],\n",
    "})\n",
    "# Define outcome space of each random variable\n",
    "outcomeSpace = dict(\n",
    "    H=(0,1),\n",
    "    L=(0,1),\n",
    "    A=(0,1),\n",
    "    V=(0,1),\n",
    "    S=(0,1),\n",
    "    T=(0,1),\n",
    "    C=(0,1,2),\n",
    "    O=(0,1,2),\n",
    "    B=(0,1,2),\n",
    ")\n",
    "# Initialize Bayesian Network object\n",
    "ICU_Net = BayesNet(graph, outcomeSpace)\n",
    "\n",
    "# Add each factor one by one to the ICU_Net object\n",
    "h = Factor(('H',), outcomeSpace)\n",
    "h[0] = 0.8\n",
    "h[1] = 0.2\n",
    "ICU_Net.factors['H'] = h\n",
    "\n",
    "v = Factor(('L', 'H', 'V'), outcomeSpace)\n",
    "v[0, 0, 0] = 0.05\n",
    "v[0, 0, 1] = 0.95\n",
    "v[0, 1, 0] = 0.99\n",
    "v[0, 1, 1] = 0.01\n",
    "v[1, 0, 0] = 0.1\n",
    "v[1, 0, 1] = 0.9\n",
    "v[1, 1, 0] = 0.9\n",
    "v[1, 1, 1] = 0.1\n",
    "ICU_Net.factors['V'] = v\n",
    "\n",
    "c = Factor(('V', 'C'), outcomeSpace)\n",
    "c[0, 0] = 0.94\n",
    "c[0, 1] = 0.04\n",
    "c[0, 2] = 0.02\n",
    "c[1, 0] = 0.02\n",
    "c[1, 1] = 0.26\n",
    "c[1, 2] = 0.72\n",
    "ICU_Net.factors['C'] = c\n",
    "\n",
    "l = Factor(('L',), outcomeSpace)\n",
    "l[0] = 0.95\n",
    "l[1] = 0.05\n",
    "ICU_Net.factors['L'] = l\n",
    "\n",
    "s = Factor(('L','H','S'), outcomeSpace)\n",
    "s[0, 0, 0] = 0.04\n",
    "s[0, 0, 1] = 0.96\n",
    "s[0, 1, 0] = 0.48\n",
    "s[0, 1, 1] = 0.52\n",
    "s[1, 0, 0] = 0.95\n",
    "s[1, 0, 1] = 0.05\n",
    "s[1, 1, 0] = 0.1\n",
    "s[1, 1, 1] = 0.9\n",
    "ICU_Net.factors['S'] = s\n",
    "\n",
    "o = Factor(('S', 'V', 'O'), outcomeSpace)\n",
    "o[0, 0, 0] = 0.97\n",
    "o[0, 0, 1] = 0.01\n",
    "o[0, 0, 2] = 0.02\n",
    "o[0, 1, 0] = 0.78\n",
    "o[0, 1, 1] = 0.19\n",
    "o[0, 1, 2] = 0.03\n",
    "o[1, 0, 0] = 0.22\n",
    "o[1, 0, 1] = 0.76\n",
    "o[1, 0, 2] = 0.02\n",
    "o[1, 1, 0] = 0.01\n",
    "o[1, 1, 1] = 0.01\n",
    "o[1, 1, 2] = 0.98  \n",
    "ICU_Net.factors['O'] = o\n",
    "\n",
    "t = Factor(('A','T'), outcomeSpace)\n",
    "t[0, 0] = 0.30\n",
    "t[0, 1] = 0.70\n",
    "t[1, 0] = 0.9\n",
    "t[1, 1] = 0.1\n",
    "ICU_Net.factors['T'] = t\n",
    "\n",
    "b = Factor(('O', 'T', 'B'), outcomeSpace)\n",
    "b[0, 0, 0] = 0.9\n",
    "b[0, 0, 1] = 0.05\n",
    "b[0, 0, 2] = 0.05\n",
    "b[0, 1, 0] = 0.30\n",
    "b[0, 1, 1] = 0.62\n",
    "b[0, 1, 2] = 0.08\n",
    "b[1, 0, 0] = 0.93\n",
    "b[1, 0, 1] = 0.06\n",
    "b[1, 0, 2] = 0.01\n",
    "b[1, 1, 0] = 0.02\n",
    "b[1, 1, 1] = 0.49\n",
    "b[1, 1, 2] = 0.49\n",
    "b[2, 0, 0] = 0.90\n",
    "b[2, 0, 1] = 0.08\n",
    "b[2, 0, 2] = 0.02\n",
    "b[2, 1, 0] = 0.01\n",
    "b[2, 1, 1] = 0.08\n",
    "b[2, 1, 2] = 0.91 \n",
    "ICU_Net.factors['B'] = b\n",
    "\n",
    "a = Factor(('A',), outcomeSpace)\n",
    "a[0] = 0.99\n",
    "a[1] = 0.01\n",
    "ICU_Net.factors['A'] = a\n",
    "\n",
    "ICU_Net.graph.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For now on, we will use the same notation as the lecture slides. In particular, the variable sigma ($\\Sigma$) represents the assignment being sampled (variable by variable) from the Bayesian network. Each time we sample a value $x$ to a variable $X$, we add the entry `X:x` to the dictionary sigma.\n",
    "\n",
    "Here is a test to confirm the `sample_variable` function is implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Test code\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "sigma = {'A': 0}\n",
    "zero = 0\n",
    "one = 0\n",
    "f = ICU_Net.factors['T']\n",
    "for i in range(1000):\n",
    "    sample = f.sample_variable('T', **sigma)\n",
    "    if sample == 0:\n",
    "        zero += 1\n",
    "    else:\n",
    "        one += 1\n",
    "print(\"P(T=0|A=0)=\",zero/(one+zero))\n",
    "print(\"P(T=1|A=0)=\",one/(one+zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see (approximately) the following output:\n",
    "\n",
    "```\n",
    "P(T=0|A=0)= 0.296\n",
    "P(T=1|A=0)= 0.704\n",
    "```\n",
    "\n",
    "Compare these numbers obtained with 1000 samples with the true probabilities in the factor $P(T|A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Simulate a Bayesian network\n",
    "\n",
    "Let's implement a procedure that simulates the Bayesian network. The simulation is the process that visits the network nodes in topological order and samples each node. At the end of the simulation, we will have a complete instantiation of the network variables.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's implement the function `simulate`. We will need the `<Graph>.topological_sort` and `<Factor>.sample_variable` functions implemented before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(BayesNet):\n",
    "    def simulate(self):\n",
    "        # Compute the topological order of G (since this function is called many times, we could alternatively precompute this order in __init__)\n",
    "        order = ... # TODO\n",
    "        # Initialize `sigma` with an empty dictionary\n",
    "        sigma = ... # TODO\n",
    "        # Let's iterate over all variables according to the order\n",
    "        for var in order:\n",
    "            # Call `sample_variable` to sample variable `var`, modifying the `sigma` assignment\n",
    "            sigma[var] = ... # TODO\n",
    "        return sigma\n",
    "\n",
    "####################\n",
    "# Test code\n",
    "\n",
    "random.seed(0)\n",
    "ICU_Net = BayesNet(ICU_Net.graph, ICU_Net.outcomeSpace, ICU_Net.factors)\n",
    "print(ICU_Net.simulate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "\n",
    "```\n",
    "{'A': 0, 'T': 1, 'H': 0, 'L': 0, 'V': 1, 'C': 2, 'S': 1, 'O': 2, 'B': 2}\n",
    "```\n",
    "\n",
    "This is one complete assignment sampled according to the underlying probability distribution specified by the Bayesian network. \n",
    "\n",
    "Let's now implement forward sampling to compute answers to prior marginal queries.\n",
    "\n",
    "# Forward sampling\n",
    "\n",
    "Forward sampling is a straightforward sampling procedure to compute prior marginal queries. Prior marginal queries are queries that do not involve evidence. Therefore, they are in the form $P(\\alpha)$ where $alpha$ is an assignment to one or more variables.\n",
    "\n",
    "In our implementation, we will specify a dictionary `alpha` that maps variables to values. For instance, if `alpha = {'A': 0})`, it means we want to compute $P(A=0)$. Similarly, if `alpha = {'A': 0, 'B': 1})`, we want to know $P(A=0,B=1)$. In addition, we will need to specify `n_samples` that is the number of samples used to estimate the probabilities.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Complete the implementation of `forward_sampling` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(BayesNet):\n",
    "    def forward_sampling(self, n_samples, alpha):\n",
    "        '''\n",
    "        This function answers queries like: What is the probability that B=1? Or B=1 and T=0?\n",
    "        `n_samples`: number of samples used to compute the probabilities\n",
    "        `alpha`: The query vars, a dictionary mapping variables to values\n",
    "        return: P(alpha)\n",
    "        '''\n",
    "        # Initialise count. The variable counts the number of samples that agree with alpha \n",
    "        count = ... # TODO\n",
    "        for i in range(n_samples):\n",
    "            # Call simulate to obtain a new assigment according to P(X)\n",
    "            sample = ... # TODO\n",
    "            # Let's test if alpha matches s. If it does, increment the count p\n",
    "            if (all(v == sample[k] for k, v in alpha.items())):\n",
    "                count += ... # TODO\n",
    "        return count/n_samples\n",
    "\n",
    "random.seed(0)\n",
    "ICU_Net = BayesNet(ICU_Net.graph, ICU_Net.outcomeSpace, ICU_Net.factors)\n",
    "\n",
    "p = ICU_Net.forward_sampling(1000, {'A':0})\n",
    "print(\"P(A=0)=\", p)\n",
    "p = ICU_Net.forward_sampling(1000, {'B':1, 'A':0})\n",
    "print(\"P(B=1,A=0)=\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output (approximately):\n",
    "\n",
    "```\n",
    "P(A=0)= 0.993\n",
    "P(B=1,A=0)= 0.183\n",
    "```\n",
    "\n",
    "## Benchmarking with random queries\n",
    "\n",
    "To benchmark our code, we need a large set of queries. As the forward sampling can only compute prior marginals, our queries need to be the form $P(\\textbf{Q=q})$ where $\\textbf{Q}$ are the query variables and $\\textbf{q}$ are the corresponding values.\n",
    "\n",
    "Next, we create a procedure that will create a set of random queries. We will use these queries to compare the sampling estimates to the true probabilities obtained with variable elimination.\n",
    "\n",
    "First, let's take a look at a helper function that creates a random assignment with a maximum number of variables specified by `n_vars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function that creates an assignment (dictionary) with at most n_vars variables\n",
    "def random_assignment(outcomeSpace, n_vars, exclude = []):\n",
    "    \"\"\"\n",
    "    argument\n",
    "    `outcomeSpace`: dictionary with variables names and corresponding valid values\n",
    "    `n_vars`: maximum number of variables in the assignment\n",
    "    `exclude`: list of variables names that cannot be included in the assignment\n",
    "    return: dictionary representing an assignment with variables as keys and their corresponding values\n",
    "    \"\"\"    \n",
    "    all_vars = [v for v in outcomeSpace.keys() if v not in exclude]\n",
    "    rvars = random.sample(all_vars, random.randint(1, n_vars))\n",
    "    return { v : random.choice(outcomeSpace[v]) for v in rvars}\n",
    "\n",
    "################\n",
    "# Test code\n",
    "\n",
    "random.seed(0)\n",
    "print(random_assignment(outcomeSpace, 5))\n",
    "print(random_assignment(outcomeSpace, 3))\n",
    "print(random_assignment(outcomeSpace, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement a function that answers the queries created by `random_assignment`. We will use the variable elimination algorithm.\n",
    "\n",
    "## Exercise \n",
    "\n",
    "Implement the `query_generator_prior_marginal`. This function calls `random_assignment` to create prior marginal queries and answers those queries with the VE algorithm. The argument `n_queries` specifies the number of queries to be created and answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_generator_prior_marginal(n_queries, bayesNet):\n",
    "    \"\"\"\n",
    "    argument\n",
    "    `n_queries`: number of queries to be created and answered\n",
    "    `factors`: a dictionary will all netowrk factors\n",
    "    `outcomeSpace`: dictionary with variables names and corresponding valid values\n",
    "    return: list of dictionaries with the query and associated probability exactly computed with the VE algorithm\n",
    "    \"\"\"\n",
    "    # query_list starts as an empty list\n",
    "    query_list = ... # TODO\n",
    "    for i in range(n_queries):\n",
    "        # Assign to alpha a random assignment created by `random_assignment`\n",
    "        alpha = random_assignment(bayesNet.outcomeSpace, 3)\n",
    "        # Compute the answer to alpha using the VE query function from week 4\n",
    "        f = ... # TODO\n",
    "        # Prepare the entries to compute the probability\n",
    "        entry = tuple(alpha[v] for v in f.domain)\n",
    "        # Store the results in query_list\n",
    "        query_list.append({'query': alpha, 'prob': f[entry]})\n",
    "    return query_list\n",
    "\n",
    "\n",
    "################\n",
    "# Test code\n",
    "\n",
    "random.seed(0)\n",
    "results = query_generator_prior_marginal(10, ICU_Net)\n",
    "for r in results:\n",
    "    print(\"P(\",r['query'],\") =\",r['prob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "\n",
    "```\n",
    "P( {'C': 1, 'H': 1} ) = 0.008638\n",
    "P( {'S': 1, 'O': 2} ) = 0.6857236800000002\n",
    "P( {'B': 0} ) = 0.3210289226056\n",
    "P( {'A': 1, 'L': 0} ) = 0.009500000000000001\n",
    "P( {'L': 1, 'B': 1} ) = 0.01971616586\n",
    "P( {'L': 1, 'T': 0, 'V': 1} ) = 0.011322000000000004\n",
    "P( {'B': 0, 'S': 0} ) = 0.07445750776239998\n",
    "P( {'C': 2} ) = 0.5526300000000001\n",
    "P( {'H': 1, 'O': 0, 'C': 2} ) = 0.0028169927999999998\n",
    "P( {'L': 0, 'V': 0} ) = 0.22609999999999997\n",
    "```\n",
    "\n",
    "Now, we will use the developed functions to create 300 queries and answer them with VE and forward sampling. We will use a scatter plot to compare the results. We have developed some code in the next cell to do this task. (May take ~ 30 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "queries = query_generator_prior_marginal(300, ICU_Net)\n",
    "x = [q['prob'] for q in queries]\n",
    "y_100 = [ICU_Net.forward_sampling(100, q['query']) for q in queries]\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(x, y_100)\n",
    "plt.title(\"100 samples\")\n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Forward sampling\")\n",
    "y_500 = [ICU_Net.forward_sampling(500, q['query']) for q in queries]\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(x, y_500)\n",
    "plt.title(\"500 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Forward sampling\")\n",
    "y_1000 = [ICU_Net.forward_sampling(1000, q['query']) for q in queries]\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(x, y_1000)\n",
    "plt.title(\"1000 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Forward sampling\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we can see that forward sampling works quite well for unconditional queries. Let's take a look at the relative errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "#queries = query_generator_prior_marginal(300, ICU_Net)\n",
    "x = [q['prob'] for q in queries]\n",
    "#y_100 = [ICU_Net.forward_sampling(100, q['query']) for q in queries]\n",
    "relative_error = [abs(x[i]-y_100[i])/x[i] for i in range(len(x)) if x[i] != 0]\n",
    "probs = [p for p in x if p != 0]\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(probs, relative_error)\n",
    "plt.title(\"100 samples\")\n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Relative error\")\n",
    "#y = [ICU_Net.forward_sampling(500, q['query']) for q in queries]\n",
    "relative_error = [abs(x[i]-y_500[i])/x[i] for i in range(len(x)) if x[i] != 0]\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(probs, relative_error)\n",
    "plt.title(\"500 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Relative error\")\n",
    "#y = [ICU_Net.forward_sampling(1000, q['query']) for q in queries]\n",
    "relative_error = [abs(x[i]-y_1000[i])/x[i] for i in range(len(x)) if x[i] != 0]\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(probs, relative_error)\n",
    "plt.title(\"1000 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Relative error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the relative errors tend to be high for small probabilities, indicating that it is challenging to be relatively accurate in rare events. \n",
    "\n",
    "# Rejection sampling\n",
    "\n",
    "Forward sampling is restricted to unconditional queries ($P(\\textbf{Q})$). If we want to compute posterior marginals, we can make a straightforward adaptation. We reject all samples that do not match the evidence. This procedure is called rejection sampling.\n",
    "\n",
    "The main difference between rejection and forward sampling is the simulation of the Bayesian network. With rejection sampling, we typically abort the simulation if we sample a value $x$ for an evidence variable $X \\in \\textbf{E}$ that do conform with the evidence $\\textbf{e}$.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's start by implementing a function to simulate the network according to rejection sampling. The function `simulate_BN_rejection` has an argument `beta` that represents an assignment with the evidence. `beta` is a dictionary with the same format as `alpha`. Our objective will be to answer queries in the format $P(\\alpha|\\beta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(BayesNet):\n",
    "    def simulate_rejection(self, beta):\n",
    "        '''\n",
    "        This procedure samples all variables in the Bayesian network. If we sample a value for a variable in beta and\n",
    "        the sampled value does not match the assignment beta then we abort the simulation and return None\n",
    "        '''\n",
    "        # Compute the topological order of G (since this function is called many times, we could alternatively precompute this order in __init__)\n",
    "        order = ... # TODO\n",
    "        # Initialize `sigma` with an empty dictionary\n",
    "        sigma = {}\n",
    "        # Let's iterate over all variables according to the order\n",
    "        for var in order:\n",
    "            # Call `sample_variable` to sample variable `var`, modifying the `sigma` assignment\n",
    "            sigma[var] = ... # TODO\n",
    "            # Test if the sampled value matches beta, if it does not abort and return None\n",
    "            if var in beta.keys() and sigma[var] != beta[var]:\n",
    "                return None\n",
    "        return sigma\n",
    "\n",
    "####################\n",
    "# Test code\n",
    "\n",
    "random.seed(0)\n",
    "ICU_Net = BayesNet(ICU_Net.graph, ICU_Net.outcomeSpace, ICU_Net.factors)\n",
    "print(ICU_Net.simulate_rejection({'A':0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "\n",
    "```\n",
    "{'A': 0, 'T': 1, 'H': 0, 'L': 0, 'V': 1, 'C': 2, 'S': 1, 'O': 2, 'B': 2}\n",
    "```\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's complete the implementation of rejection sampling. The function `rejection_sampling` is very similar to `forward_sampling`. It will take `alpha` and `beta` assignments as input and return an estimate of $P(\\alpha|\\beta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(BayesNet):\n",
    "    def rejection_sampling(self, n_samples, alpha, beta):\n",
    "        '''\n",
    "        This function answers queries like: What is the probability that B=1? Or B=1 and T=0?\n",
    "        `n_samples`: number of samples used to compute the probabilities\n",
    "        `alpha`: The query vars, a dictionary mapping variables to values\n",
    "        `beta`: a dictionary maping variables to values    \n",
    "        return: P(alpha|beta)\n",
    "        '''\n",
    "        # Initialise count. The variable counts the number of samples that agree with alpha \n",
    "        count = 0\n",
    "        # n stores the number of samples that match beta\n",
    "        n = 0\n",
    "        for i in range(n_samples):\n",
    "            # Call simulate to obtain a new assigment according to P(X)\n",
    "            sample = ... # TODO\n",
    "            if sample is not None:\n",
    "                # Let's test if alpha matches s. If it does, increment the count p\n",
    "                if (all(v == sample[k] for k, v in alpha.items())):\n",
    "                    count += 1\n",
    "                n += 1\n",
    "\n",
    "        # If P(beta) is small than we may end up with no samples matching beta. We need to avoid a division by zero here\n",
    "        # This line may be a problem when p(beta) = 0 and p(alpha|beta) is undefined\n",
    "        if n == 0:\n",
    "            return 0,0\n",
    "        return count/n, n\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "ICU_Net = BayesNet(ICU_Net.graph, ICU_Net.outcomeSpace, ICU_Net.factors)\n",
    "\n",
    "p,n = ICU_Net.rejection_sampling(1000, {'T':0}, {'A':0})\n",
    "print(\"P(T=0|A=0)=\", p, \"computed with\", n, \"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see (approximately) the following output:\n",
    "\n",
    "```\n",
    "P(T=0|A=0)= 0.3021148036253776\n",
    "```\n",
    "\n",
    "Now, we can test the rejection sampling to see if it performs well for a large number of queries. We will create random queries as we did for the forward sampling.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's implement `query_generator_posterior_marginal` that will generate random queries with an alpha and beta parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query_generator_posterior_marginal(n_queries, bayesNet):\n",
    "    \"\"\"\n",
    "    argument\n",
    "    `n_queries`: number of queries to be created and answered\n",
    "    `factors`: a dictionary will all netowrk factors\n",
    "    return: list of dictionaries with the query and associated probability exactly computed with the VE algorithm\n",
    "    \"\"\"\n",
    "    # query_list starts as an empty list\n",
    "    query_list = []\n",
    "    for i in range(n_queries):\n",
    "        # Assign to alpha a random assignment created by `random_assignment`\n",
    "        alpha = ... # TODO\n",
    "        beta = ... # TODO\n",
    "        # Compute the answer to alpha using VE query function\n",
    "        f = ... # TODO\n",
    "        # Prepare the entries to compute the probability\n",
    "        entry = tuple(alpha[v] for v in f.domain)\n",
    "        # Store the results in query_list\n",
    "        query_list.append({'query': alpha, 'evidence': beta, 'prob': f[entry]})\n",
    "    return query_list\n",
    "\n",
    "\n",
    "################\n",
    "# Test code\n",
    "\n",
    "random.seed(0)\n",
    "ICU_Net = BayesNet(ICU_Net.graph, ICU_Net.outcomeSpace, ICU_Net.factors)\n",
    "results = query_generator_posterior_marginal(10, ICU_Net)\n",
    "for r in results:\n",
    "    print(\"P(\",r['query'],\" | \",r['evidence'],\") =\",r['prob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "\n",
    "```\n",
    "P( {'C': 1, 'H': 1}  |  {'B': 1, 'V': 1} ) = 0.0017515427949926759\n",
    "P( {'V': 0, 'A': 0, 'O': 2}  |  {'C': 0, 'B': 1} ) = 0.0038999772774970264\n",
    "P( {'L': 1}  |  {'A': 1, 'V': 1} ) = 0.048626626363516896\n",
    "P( {'V': 1, 'O': 0, 'B': 2}  |  {'H': 1} ) = 0.00028619778399999993\n",
    "P( {'H': 1, 'O': 0, 'C': 2}  |  {'B': 0, 'L': 0} ) = 0.004353998604346179\n",
    "P( {'A': 1}  |  {'L': 1} ) = 0.01\n",
    "P( {'O': 2, 'L': 1, 'A': 0}  |  {'S': 1, 'C': 1, 'V': 0} ) = 0.0011525190754039499\n",
    "P( {'C': 0, 'T': 1, 'S': 0}  |  {'L': 0} ) = 0.0634793472\n",
    "P( {'S': 0, 'O': 2, 'H': 0}  |  {'L': 0} ) = 0.000944\n",
    "P( {'B': 2, 'C': 2, 'O': 1}  |  {'L': 1, 'T': 1, 'V': 1} ) = 0.06239792432432433\n",
    "```\n",
    "\n",
    "Now, we can assess the performance of rejection sampling using a set of random queries. We will use 300 queries in total. The next cell computes the queries and plots the results for VE and rejection sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "queries = query_generator_posterior_marginal(300, ICU_Net)\n",
    "x = [q['prob'] for q in queries]\n",
    "r_100 = [ICU_Net.rejection_sampling(100, q['query'], q['evidence']) for q in queries]\n",
    "y_100 = [r[0] for r in r_100]\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(x, y_100)\n",
    "plt.title(\"100 samples\")\n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Rejection sampling\")\n",
    "r_500 = [ICU_Net.rejection_sampling(500, q['query'], q['evidence']) for q in queries]\n",
    "y_500 = [r[0] for r in r_500]\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(x, y_500)\n",
    "plt.title(\"500 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Rejection sampling\")\n",
    "r_1000 = [ICU_Net.rejection_sampling(1000, q['query'], q['evidence']) for q in queries]\n",
    "y_1000 = [r[0] for r in r_1000]\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(x, y_1000)\n",
    "plt.title(\"1000 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Rejection sampling\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look much worse than the ones we obtained with forward sampling. This illustrates the issue we described in the lectures with the difficulty of sampling from distributions in the form $P(.|\\beta)$. When $P(\\beta)$ is small, we reject many samples, and we may end up with poor estimates.\n",
    "\n",
    "Let's confirm this hypothesis. The next cell computes the same plot, but it associates the size of each point with the number of samples used to calculate the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "#queries = query_generator_posterior_marginal(300, ICU_Net)\n",
    "x = [q['prob'] for q in queries]\n",
    "#r_100 = [ICU_Net.rejection_sampling(100, q['query'], q['evidence']) for q in queries]\n",
    "#y_100 = [x[0] for x in r_100]\n",
    "w = [x[1]+.1 for x in r_100]                                   # Don't let the zero weight estimates disapper in the plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(x, y, s = w)\n",
    "plt.title(\"100 samples\")\n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Rejection sampling\")\n",
    "#r_500 = [ICU_Net.rejection_sampling(500, q['query'], q['evidence']) for q in queries]\n",
    "#y_500 = [x[0] for x in r_500]\n",
    "w = [x[1]+.1 for x in r_500]                                   # Don't let the zero weight estimates disapper in the plot\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(x, y, s = w)\n",
    "plt.title(\"500 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Rejection sampling\")\n",
    "#r_1000 = [ICU_Net.rejection_sampling(1000, q['query'], q['evidence']) for q in queries]\n",
    "#y_1000 = [x[0] for x in r_1000]\n",
    "w = [x[1]+.1 for x in r_1000]                                   # Don't let the zero weight estimates disapper in the plot\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(x, y, s = w)\n",
    "plt.title(\"1000 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Rejection sampling\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the poorest estimates are those associated with small sample sizes. Notice the several bad estimates returned zero probability, but the true probability was not zero. These are cases the probability of evidence was tiny, and we ended up with no samples.\n",
    "\n",
    "# Likelihood weighting\n",
    "\n",
    "Let's see if we can do better with likelihood weighting sampling. We need to start by creating a new procedure to simulate the Bayesian network and compute the weights.\n",
    "\n",
    "The procedure is simple. We traverse the network in topological order. If the current node is an evidence variable, then we compute the weight associated with its likelihood. If the current node is not in the evidence set, we sample the variable related to the node.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's implement the function `simulate_BN_likelihood` that simulates the network and computes a sample and its corresponding weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(BayesNet):\n",
    "    def simulate_likelihood(self, beta):\n",
    "        \"\"\"\n",
    "        `beta`:  a dictionary that represents an assignment for all evidence variables\n",
    "        return: `sigma`, weight: a dictionary with a complete assigment with all network variables respecting beta and its corresponding weight\n",
    "        \"\"\"\n",
    "        # Call topologicalSort to compute the order we will traverse the network\n",
    "        order = ... # TODO\n",
    "        # sigma is a complete assignment for all network variables. We start with an empty assignment\n",
    "        sigma = {}\n",
    "        # This is the assignment weight. We start with one and change it for each observed variable\n",
    "        w = ... # TODO\n",
    "        for var in order:\n",
    "            if var not in beta.keys():\n",
    "                # For the variables NOT in the evidence (beta), we need to sample a value for the variable\n",
    "                sigma[var] = ... # TODO\n",
    "            else:\n",
    "                # For the variables in the evidence (beta), we need to update the weight\n",
    "                sigma[var] = ... # TODO (use beta to get the value of this var)\n",
    "                entry = tuple(sigma[v] for v in self.factors[var].domain)\n",
    "                w *= self.factors[var][entry]\n",
    "\n",
    "        return sigma, w\n",
    "        \n",
    "\n",
    "####################\n",
    "# Test code\n",
    "\n",
    "ICU_Net = BayesNet(ICU_Net.graph, ICU_Net.outcomeSpace, ICU_Net.factors)\n",
    "random.seed(0)\n",
    "print(ICU_Net.simulate_likelihood({'A':0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "\n",
    "```\n",
    "({'A': 0, 'T': 1, 'H': 0, 'L': 0, 'V': 1, 'C': 2, 'S': 1, 'O': 2, 'B': 2}, 0.99)\n",
    "```\n",
    "\n",
    "Now, we can implement the `likelihood_weighting` function. This subroutine samples the network by simulating it according to the `simulate_BN_likelihood` procedure. It will be very similar to the code we have developed for the forward and rejection sampling.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's implement the `likelihood_weighting` function that creates `n_samples` samples and estimates the probability according to those samples.\n",
    "\n",
    "Notice that the samples now have weights and we must consider them when we compute the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(BayesNet):\n",
    "    def likelihood_weighting(self, n_samples, alpha, beta):\n",
    "        '''\n",
    "        This function answers queries like: What is the probability that B=1? Or B=1 and T=0?\n",
    "        `n_samples`: number of samples used to compute the probabilities\n",
    "        `alpha`: The query vars, a dictionary mapping variables to values\n",
    "        `beta`: a dictionary maping variables to values    \n",
    "        return: P(alpha|beta)\n",
    "        '''\n",
    "        # p stores the sum of weights of samples that match alpha and beta    \n",
    "        p = 0\n",
    "        # n stores the sum of weights of samples that match beta    \n",
    "        n = 0\n",
    "        for i in range(n_samples):\n",
    "            # Call simulate_BN_likelihood to simulate the Bayesian network according to the likelihood weighting procedure        \n",
    "            s, w = ... # TODO\n",
    "            # Check if the sample matches alpha        \n",
    "            if (all(v == s[k] for k, v in alpha.items())):\n",
    "                p += w\n",
    "            n += w\n",
    "        # If P(beta) is small than we may end up with no samples matching beta. We need to avoid a division by zero here\n",
    "        # This line may be a problem when p(beta) = 0 and p(alpha|beta) is undefined        \n",
    "        if n == 0:\n",
    "            return (0, 0)\n",
    "        return p/n, n\n",
    "\n",
    "#####################\n",
    "# Test code\n",
    "\n",
    "random.seed(0)\n",
    "ICU_Net = BayesNet(ICU_Net.graph, ICU_Net.outcomeSpace, ICU_Net.factors)\n",
    "\n",
    "p,w = ICU_Net.likelihood_weighting(1000, {'T':0}, {'A':0})\n",
    "print(\"P(T=0|A=0)=\", p, \"computed with weight\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "\n",
    "```\n",
    "P(A=0|T=0)= 0.2879999999999989 computed with weight 990.0000000000076\n",
    "```\n",
    "\n",
    "Now, we have all the tools we need to build several random queries and answer them with likelihood weighting. Let's see how this technique compares to rejection sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "queries = query_generator_posterior_marginal(300, ICU_Net)\n",
    "x = [q['prob'] for q in queries]\n",
    "l_100 = [ICU_Net.likelihood_weighting(100, q['query'], q['evidence']) for q in queries]\n",
    "y_100 = [l[0] for l in l_100]\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"100 samples\")\n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Likelihood weighting\")\n",
    "l_500 = [ICU_Net.likelihood_weighting(500, q['query'], q['evidence']) for q in queries]\n",
    "y_500 = [l[0] for l in l_500]\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(x, y_500)\n",
    "plt.title(\"500 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Likelihood weighting\")\n",
    "l_1000 = [ICU_Net.likelihood_weighting(1000, q['query'], q['evidence']) for q in queries]\n",
    "y_1000 = [l[0] for l in l_1000]\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(x, y_1000)\n",
    "plt.title(\"1000 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Likelihood weighting\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Likelihood weighting is a lot better than rejection sampling. That explains why this technique is so popular and used in several tasks such as robot mapping as we have seen in the lectures.\n",
    "\n",
    "We can make a similar analysis we did with rejection sampling and check if the worst estimates are associated with small weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "#queries = query_generator_posterior_marginal(300, ICU_Net)\n",
    "x = [q['prob'] for q in queries]\n",
    "#l_100 = [ICU_Net.likelihood_weighting(100, q['query'], q['evidence']) for q in queries]\n",
    "#y_100 = [x[0] for x in r_100]\n",
    "w_100 = [x[1]+.1 for x in l_100]                                   # Don't let the zero weight estimates disapper in the plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(x, y_100, s = w_100)\n",
    "plt.title(\"100 samples\")\n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Likelihood weighting\")\n",
    "#l_500 = [ICU_Net.likelihood_weighting(500, q['query'], q['evidence']) for q in queries]\n",
    "#y_500 = [x[0] for x in l_500]\n",
    "w_500 = [x[1]+.1 for x in l_500]                                   # Don't let the zero weight estimates disapper in the plot\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(x, y_500, s = w_500)\n",
    "plt.title(\"500 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Likelihood weighting\")\n",
    "#l_1000 = [ICU_Net.likelihood_weighting(1000, q['query'], q['evidence']) for q in queries]\n",
    "#y_1000 = [x[0] for x in l_1000]\n",
    "w_1000 = [x[1]+.1 for x in l_1000]                                   # Don't let the zero weight estimates disapper in the plot\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(x, y_1000, s = w_1000)\n",
    "plt.title(\"1000 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Likelihood weighting\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs sampling\n",
    "\n",
    "Gibbs sampling is an algorithm based on Markov chains. The basic idea is that we can create a transition matrix that has as a stationary distribution the distribution induced by the Bayesian or Markov network. If we simulate this chain long enough, we can sample from the chain as if we were sampling from the underlying distribution induced by the network.\n",
    "\n",
    "Gibbs sampling implementation is much more complex than the previous methods. To be an efficient method, we need to implement the inference for the transition matrix carefully.\n",
    "\n",
    "In particular, such inference will involve a reduced number of factors. When sampling a variable $X$ we will need to have access to all factors that mention $X$.\n",
    "\n",
    "We start by creating a graph that we call the *Gibbs graph*. In this graph, a node $X$ has a direct edge to all nodes $Y_i$ in which $X$ appears in the factor associated with $Y_i$.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Implement the `create_gibbs_graph` function that takes as input a set of factors and outputs a Gibbs graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class BayesNet(BayesNet):\n",
    "    def create_gibbs_graph(self):\n",
    "        \"\"\"\n",
    "        argument \n",
    "        Returns an Graph object of the Gibbs graph\n",
    "        \"\"\"        \n",
    "        # create a deepcopy of this network graph\n",
    "        gg = ... # TODO\n",
    "        for var in gg:\n",
    "            ... # add an edge from this node to itself\n",
    "        return gg\n",
    "\n",
    "\n",
    "##############\n",
    "# Test code\n",
    "\n",
    "# List of positions for each node\n",
    "# We use node positions to replicate the previous figure\n",
    "# This can be removed if the nodes do not need to be presented in a specific order\n",
    "pos = {\n",
    "    'B': '1,0!',\n",
    "    'O': '0,1!',\n",
    "    'C': '1,1!',\n",
    "    'T': '2,1!',\n",
    "    'S': '0,2!',\n",
    "    'V': '1,2!',\n",
    "    'A': '2,2!',\n",
    "    'L': '0,3!',\n",
    "    'H': '1,3!',\n",
    "}\n",
    "\n",
    "ICU_Net = BayesNet(ICU_Net.graph, ICU_Net.outcomeSpace, ICU_Net.factors)\n",
    "gg = ICU_Net.create_gibbs_graph()\n",
    "gg.show(positions=pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your output is correct, you should generate the following graph.\n",
    "\n",
    "![Gibbs graph](img/GG.png)\n",
    "\n",
    "The Gibbs graph tells us that the factors that mention, for example, $H$ are in nodes $H$, $S$ and $V$. Therefore, we will need to multiply these factors to sample the variable $H$.\n",
    "\n",
    "Before we implement the Gibbs transition, we need to remember that Markov chain Monte Carlo (MCMC) methods work with complete assignments. We will need a helper function that creates a complete assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(BayesNet):\n",
    "    def complete_assignment(self, exclude = []):\n",
    "        return { v : random.choice(outcomeSpace[v]) for v in outcomeSpace.keys() if v not in exclude}\n",
    "\n",
    "\n",
    "#####################\n",
    "# Test code\n",
    "\n",
    "random.seed(0)\n",
    "ICU_Net = BayesNet(ICU_Net.graph, ICU_Net.outcomeSpace, ICU_Net.factors)\n",
    "alpha = ICU_Net.complete_assignment()\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement a `GibbsSampler` class with a `gibbs_transition` function. In some sense, it is similar to simulating the network. The main differences are:\n",
    "\n",
    "1. We will work with complete assignments, instead of building one from scratch. \n",
    "2. There is no need to traverse the network in any specific order.\n",
    "3. Each time we sample a variable, we will choose the variable randomly from the set of all non-evidence variables in the network.\n",
    "4. The sampling process will involve an inference step. The previous sampling methods we discussed were more straightforward since we used conditional probabilities already available in the network parameters.\n",
    "\n",
    "The main challenge is to implement the inference step efficiently. We will use the Gibbs graph to help us to identify the relevant factors correctly.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Implement the `__init__` and `gibbs_transition` function. The transition function takes the state as input, chooses one variable randomly with uniform distribution and samples a new value for this variable according to the Gibbs transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GibbsSampler():\n",
    "    def __init__(self, bayesNet, beta):\n",
    "        self.bayesNet = bayesNet\n",
    "        self.beta = beta\n",
    "        self.gibbs_graph = bayesNet.create_gibbs_graph()\n",
    "        self.outcomeSpace = list(bayesNet.factors.values())[0].outcomeSpace\n",
    "\n",
    "        # initialise state\n",
    "        self.state = bayesNet.complete_assignment(beta)\n",
    "        self.state.update(beta)\n",
    "\n",
    "\n",
    "    def gibbs_transition(self):\n",
    "        # These are the non-evidence variables in the network. We will sample from this list\n",
    "        not_ev_vars = [v for v in self.outcomeSpace.keys() if v not in self.beta.keys()]\n",
    "        # Randomly sample one variable with uniform distribution\n",
    "        X = random.choice(not_ev_vars)\n",
    "        # Create an empty factor that we will use to store our inference results\n",
    "        gibbs_factor = Factor((X,), self.outcomeSpace)\n",
    "        for value in self.outcomeSpace[X]:\n",
    "            # Let's use the Gibbs graph to see which factors we need to multiply\n",
    "            for var in self.gibbs_graph.children(X):\n",
    "                # IMPORTANT: instead of call join and multiply all factors, we will only multiply the entries compatible with evidence\n",
    "                # Remember we are working with complete assignments. See the lecture about classification with graphical models for details\n",
    "                \n",
    "                # Assign a value to variable X in the state\n",
    "                self.state[X] = value\n",
    "                # Compute the entry to retrieve the probability \n",
    "                entry = tuple(self.state[v] for v in self.bayesNet.factors[var].domain)\n",
    "                # Multiply the probabilities in the relevant factors\n",
    "                gibbs_factor[value] *= ... # TODO\n",
    "        # Renormalize the gibbs_factor, so we probabilities sum to one and we can sample a value for X\n",
    "        gibbs_factor = ... # TODO\n",
    "        # Call sample_variable to sample a value for X according to the distribution in gibbs_factor\n",
    "        self.state[X] = gibbs_factor.sample_variable(X)\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    \n",
    "##################\n",
    "# Test code\n",
    "random.seed(6)\n",
    "# create a random query\n",
    "beta = random_assignment(outcomeSpace, 3)\n",
    "# initialize sampler\n",
    "sampler = GibbsSampler(ICU_Net, beta)\n",
    "print(sampler.state)\n",
    "sampler.gibbs_transition()\n",
    "print(sampler.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your response is correct, you should see the following output:\n",
    "\n",
    "```\n",
    "{'H': 0, 'A': 1, 'V': 1, 'S': 1, 'T': 0, 'B': 1, 'L': 1, 'O': 0, 'C': 0}\n",
    "{'H': 0, 'A': 1, 'V': 1, 'S': 0, 'T': 0, 'B': 1, 'L': 1, 'O': 0, 'C': 0}\n",
    "```\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's replace our current initialisation method with a slightly different way to initialise the chains. We can use the `simulate_likelihood` function to generate a sample that conforms with the evidence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GibbsSampler(GibbsSampler):\n",
    "    def __init__(self, bayesNet, beta):\n",
    "        self.bayesNet = bayesNet\n",
    "        self.beta = beta\n",
    "        self.gibbs_graph = bayesNet.create_gibbs_graph()\n",
    "        self.outcomeSpace = list(bayesNet.factors.values())[0].outcomeSpace\n",
    "\n",
    "        # Call simulate_likelihood to create initial assignment that agrees with beta\n",
    "        w = 0\n",
    "        # Let's discard assigments with zero weights. They may be invalid states if we have deterministic CPTs\n",
    "        while(w==0):\n",
    "            self.state, w = ... # TODO\n",
    "\n",
    "        \n",
    "################\n",
    "# Test code\n",
    "\n",
    "random.seed(0)\n",
    "beta = random_assignment(outcomeSpace, 3)\n",
    "for i in range(5):\n",
    "    sampler = GibbsSampler(ICU_Net, beta)\n",
    "    print(sampler.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "\n",
    "```\n",
    "[{'A': 0, 'T': 1, 'H': 1, 'L': 0, 'V': 0, 'C': 1, 'S': 1, 'O': 1, 'B': 1},\n",
    " {'A': 0, 'T': 1, 'H': 1, 'L': 0, 'V': 0, 'C': 1, 'S': 1, 'O': 2, 'B': 2},\n",
    " {'A': 0, 'T': 1, 'H': 1, 'L': 0, 'V': 0, 'C': 1, 'S': 1, 'O': 1, 'B': 1},\n",
    " {'A': 0, 'T': 1, 'H': 1, 'L': 0, 'V': 0, 'C': 1, 'S': 0, 'O': 0, 'B': 0},\n",
    " {'A': 0, 'T': 1, 'H': 1, 'L': 0, 'V': 0, 'C': 1, 'S': 0, 'O': 0, 'B': 1}]\n",
    "```\n",
    "\n",
    "Finally, we implement the function `gibbs_sampling`. This function has two unique arguments. The first, `n_burnin`, is the number of transitions we apply to the chain before we start sampling. This period is known as burn-in. The second, `n_steps`, is the number of transitions we apply to the chain between two consecutive samples. `n_steps` help us to obtain samples that are less correlated between each other.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's implement `gibbs_sampling`. We prepared a stub for you. You need to fill in the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampling(bayesNet, alpha, beta, n_chains, n_samples, n_burnin=1000, n_steps=1):\n",
    "    \"\"\"\n",
    "    argument\n",
    "    `alpha`:  a dictionary that represents an assignment for all network variables\n",
    "    `beta`:  a dictionary that represents an assignment for all evidence variables\n",
    "    `n_samples`: number of samples to draw from each chain\n",
    "    `n_burnin`: number of burn-in steps\n",
    "    `n_steps`: number of steps (transitions) between two consecutive samples\n",
    "    return: an estimate of P(alpha|beta) according Gibbs sampling\n",
    "    \"\"\"\n",
    "    # initialise sampling chains\n",
    "    chains = [None]*n_chains\n",
    "    for i in range(n_chains):\n",
    "        chains[i] = ... # TODO initialise chains\n",
    "\n",
    "    ########### Start of the Burn-in phase ###########\n",
    "    for i in range(n_burnin):\n",
    "        for chain in chains:\n",
    "            # Call gibbs_transition chain\n",
    "            ... # TODO\n",
    "    ############ End of the Burn-in phase ############\n",
    "\n",
    "    # Number of samples that match the alpha and beta assignments\n",
    "    p = 0\n",
    "\n",
    "    # Now we are going to start sampling\n",
    "    for i in range(n_samples):\n",
    "        for chain in chains:\n",
    "            for s in range(n_steps):\n",
    "                # Call gibbs_transition\n",
    "                state = ... # TODO\n",
    "            # Check if the new samples matches alpha\n",
    "            if (all(v == state[k] for k, v in alpha.items())):\n",
    "                p += 1\n",
    "    return p/(n_samples*n_chains)\n",
    "                \n",
    "#######################\n",
    "# Test code\n",
    "random.seed(0)\n",
    "p = gibbs_sampling(ICU_Net, {'T':0}, {'A':0}, 10, 1000, n_burnin=1000,n_steps=1)\n",
    "print(\"P(T=0|A=0)=\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should see the following output:\n",
    "\n",
    "```\n",
    "P(A=0|T=0)= 0.3411\n",
    "```\n",
    "\n",
    "Now, let's compare Gibbs sampling with the remaining sampling methods. To make things a little more comparable, we will use a single chain. Our burn-in period will be 10 samples, and we will sample every 10 chain transitions.\n",
    "\n",
    "The next cell should take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "ICU_Net = BayesNet(ICU_Net.graph, ICU_Net.outcomeSpace, ICU_Net.factors)\n",
    "queries = query_generator_posterior_marginal(300, ICU_Net)\n",
    "x = [q['prob'] for q in queries]\n",
    "y = [gibbs_sampling(ICU_Net, q['query'], q['evidence'], 1, 100, 10, 10) for q in queries]\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"100 samples\")\n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Gibbs sampling\")\n",
    "y = [gibbs_sampling(ICU_Net, q['query'], q['evidence'], 1, 500, 10, 10) for q in queries]\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"500 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Gibbs sampling\")\n",
    "y = [gibbs_sampling(ICU_Net, q['query'], q['evidence'], 1, 1000, 10, 10) for q in queries]\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"1000 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Gibbs sampling\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. It seems that Gibbs sampling can provide competitive results if compared to likelihood sampling. However, Gibbs sampling is significatively more demanding. \n",
    "\n",
    "We have worked with a simple network. In more complex ones, likelihood sampling may have issues and end up generating samples with small weights. In this case, Gibbs sampling may perform better.\n",
    "\n",
    "That is all for today. We have reached the end of week 10. It was good to have you with us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge exercise: Vectorized likelihood sampling (optional)\n",
    "\n",
    "It's possible to make likelihood weighting queries much faster, by vectorising all calculations with numpy. This works by iterating over each variable, and generating all the randomly sampled outcomes for that variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(BayesNet):\n",
    "    def fast_likelihood_weighting(self, n_samples, alpha, beta):\n",
    "        '''\n",
    "        vectorised likelihood weighting sampler\n",
    "        '''\n",
    "        # Call topologicalSort to compute the order we will traverse the network\n",
    "        order = self.graph.topological_sort()\n",
    "        # init vector of weights (ones)\n",
    "        weights = np.ones((n_samples,))\n",
    "        # init samples dict of arrays \n",
    "        samples = {}\n",
    "        for var in order:\n",
    "            f = self.factors[var]\n",
    "            if var not in beta:\n",
    "                # Sample a vector of outcomes (length n_samples)\n",
    "                ... # TODO >8 lines of numpy\n",
    "                samples[var] = ... # TODO output should be a vector of outcome indicies sampled from this variable (conditional on parents)\n",
    "            else:\n",
    "                # set the outcome of this var to whatever it says in `beta`\n",
    "                outcomeIndex = f.outcomeSpace[var].index(beta[var])\n",
    "                samples[var] = outcomeIndex*np.ones((n_samples,), dtype=np.int)\n",
    "                # what does this var depend on?\n",
    "                # select those columns of the output vectors \n",
    "                indicies = tuple(samples[v] for v in f.domain)\n",
    "                # use those columns to index into the vector\n",
    "                probs_vector = f.table[indicies]\n",
    "                # multiply this vector with the weights vector\n",
    "                weights *= probs_vector\n",
    "\n",
    "        # now select rows that match alpha\n",
    "        matchesAlpha = np.ones(n_samples, dtype=np.bool_)\n",
    "        for var, outcome in alpha.items():\n",
    "            matchesAlpha &= (samples[var] == outcome)\n",
    "        # sum up the weights of those rows\n",
    "        alphaWeightSum = np.sum(weights*matchesAlpha)\n",
    "        # divide by sum of all weights\n",
    "        totalSum = np.sum(weights)\n",
    "        return alphaWeightSum/totalSum, totalSum\n",
    "\n",
    "#####################\n",
    "# Test code\n",
    "\n",
    "np.random.seed(0)\n",
    "ICU_Net = BayesNet(ICU_Net.graph, ICU_Net.outcomeSpace, ICU_Net.factors)\n",
    "\n",
    "p,w = ICU_Net.fast_likelihood_weighting(1000, {'T':0}, {'A':0})\n",
    "print(\"P(T=0|A=0)=\", p, \"computed with weight\", w)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "queries = query_generator_posterior_marginal(300, ICU_Net)\n",
    "x = [q['prob'] for q in queries]\n",
    "y = [ICU_Net.fast_likelihood_weighting(1000, q['query'], q['evidence'])[0] for q in queries]\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"1000 samples\")\n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Likelihood weighting\")\n",
    "y = [ICU_Net.fast_likelihood_weighting(5000, q['query'], q['evidence'])[0] for q in queries]\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"5000 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Likelihood weighting\")\n",
    "y = [ICU_Net.fast_likelihood_weighting(10000, q['query'], q['evidence'])[0] for q in queries]\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"10000 samples\")    \n",
    "plt.xlabel(\"True probabilities\")\n",
    "plt.ylabel(\"Likelihood weighting\")    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "198px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "783px",
    "left": "0px",
    "right": "1346.87px",
    "top": "108px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
